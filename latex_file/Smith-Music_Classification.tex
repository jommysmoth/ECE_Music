\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,   
    urlcolor=red,
}
 
\urlstyle{same}
\title{Music Classification}
\author{James Smith}
\date{April 2018}

\begin{document}

\maketitle

\section{Introduction}
When listening to music, not only do we have an intimate understanding of what we like, it is also very simple differentiate by genre. The more we listen to a type of music, the better we get at determining the subtle differences between these genres. In general, these differences can be described, for example, post-rock music is known to have more smoother, building tones, while something like hardcore is known for more fast paced, harsh tones. In other instances, trying to categorize a song can be very difficult though. Genres like experimental often have a vague meaning, thus are more difficult to describe. This project's goal is to use a deep learning technique in order to accurately categorize songs by their genres. Specifically, my goal is to train a convolutional neural network to classify songs by their genre. The type of data available changes how this network is constructed, so it important to understand the domain.
\subsection{Digital Representations of Sounds}
 A song is represented by data points denoting the pitches over the time length of the song, at a defined rate, often called the sampling rate.  This sample rate for most songs is either 44.1 KHz or 96KHz. There are two channels of these pitch sequences, which are the left and right channels of the audio. In your headset, this mixing of audio from both speakers create the experience of the song we know. This effect is similar to the RGB channels of a photograph, where each pixel is a 3-dimensional value that completes the representation of the image. This comparison to images will be used a later part of this paper to explain the use of a CNN for this classification. The accuracy of these samples are denoted by the bit-depth value for the song, which is telling of how good of a bit representation it is.
\subsection{Potential Data Issues}
The issue with this type of data is also part of its strength, which is the overwhelming amount of features to deal with (3 minute song at 44.1KHz has about $10^{7}$ features). This potentially gives a lot of insight into each sample, if these contain important trends, but can also create insanely high running times for the algorithm which don't end up helping the learning. Another major issue is the input size for each song. Between different sampling rates and song time, the sequence lengths of each song is are greatly different, which most networks can't handle. Usually, networks that do handle this (commonly recurrent neural networks), would have the greatest issue with high feature amounts without any additional dimensional reduction. The aim for this CNN training is to keep a high amount of features available, while still being able to train on large sets of data.
\section{Processing the Song Data}
\subsection{Song Clips}
Since the songs can range from a minute to a half hour, creating some predetermined size to clip the song to is important. Even though we lose some features in this process, intuitively it seems like the right way to reduce data dimensionality. If I was to reduce the data points over the whole set, the data would have to be reduced by different amounts depending on the overall size of the song. This means the longer the song, the more "choppy" the signal would become. This means each song is changed in a dynamically different way, which could negatively impact the network's learning. By taking an x many features from some portion of the song (in my case somewhere halfway through the song), the input size is set and every sample has the same sequence length. The only issue with this technique is if two songs use different sampling rates, the length of the clip in seconds for them is different. Though this is an issue, if you listen to a song for 3 seconds or 5 seconds, it is equally easy to classify a song. My hope is that this will reflect in the test results from the data. 
\subsection{Audio Spectrogram}
The way in which a network perceives our features determines how well it train on them and eventually classify. Now that we have every song as an equally sized sequence, the question remains if there is anything left to do that would create a better representation of the data set for the network. Earlier due to the multi-channel nature of the data, this sequence was compared to idea of an unfolded image. So, to create the image version of this data, it could make sense to fold the sequence into an image by cutting the signal every x amount of points (if x divided by the length of the sequence is an integer y), and creating a x by y by 2 channel image. Although this could potentially work, shapes and other regions created by this are most likely not consistent over the course of the genres, meaning that the network has nothing to learn. For this reason, I chose to turn an image into a spectrogram. The process of making a spectrogram starts with moving a window over the sequence and take a Fourier transform of the windowed area. This give an amplitude value for every frequency, and x many of these sequences if there are x many windows used on the sequence. This creates an image of frequency vs. time, with a 2-dimensional value for every "pixel", for the amplitude in each channel. This process will hopefully create similar regions within genres, and cause the CNN to train easily.
\subsection{The Convolutional Neural Network}
This is the part of the project I have worked on only slightly so far. Right now, I have a basic CNN architecture, with a conv - pool - conv - pool, then into two fully connected labels for output, all with ReLu for activation functions. For backwards propagation, it utilizes a stochastic gradient descent, on a batch size of four samples. So far loss is not decreasing at a great rate, so normalization of the "images" is the next step in the training of the network. 
\subsection{What I've Done So Far}
In the repository \href{https://github.com/jommysmoth/ECE_Music}{https://github.com/jommysmoth/ECE Music} you can see what I have currently been working on in the progress for the project. The only thing not included is the data set, since the storage space necessary is too big. All of the data preparation code is done, I just need to add a bigger collection of songs into the necessary folder to ensure there is a enough data to adequately train the CNN. So far, the last thing I've done is just get it so the training data is put through the network, and calculating loss.
\subsection{Plans To Do Next}
The normalization of the channels is the next thing to do, in order to decrease overall loss on the learning process. After that, my goal is to test the generalization of the trained network, in order to see how the network preforms. If it eventually preforms within a good accuracy, my final plan is to use some visualization to best present how the songs are being selected by the network to be certain classes.
\end{document}
